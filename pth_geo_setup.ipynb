{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pytorch_geo_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salimbeni1/DB-IntroProject/blob/master/pth_geo_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3VdLQPk1JMr"
      },
      "source": [
        "{{ badge }}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqjoXKVv3SrB",
        "outputId": "ded5c30e-a633-465b-95f0-eede604824d8"
      },
      "source": [
        "! python -c \"import torch; print('cuda version : ' , torch.version.cuda)\"\r\n",
        "! python -c \"import torch; print('pytorch version : ' ,torch.__version__)\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda version :  10.1\n",
            "pytorch version :  1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqlQN1vi43As"
      },
      "source": [
        "make sure that you install a compatible pytorch geometric version, here for example for 1.7.0+cu101 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv8a_aEX4GNv",
        "outputId": "6258af43-1b18-41e2-881f-1fe10fb3b60f"
      },
      "source": [
        "%%bash\r\n",
        "pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\r\n",
        "pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\r\n",
        "pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\r\n",
        "pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\r\n",
        "pip install torch-geometric"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.7.0%2Bcu101/torch_scatter-2.0.5-cp36-cp36m-linux_x86_64.whl (11.9MB)\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.7.0%2Bcu101/torch_sparse-0.6.8-cp36-cp36m-linux_x86_64.whl (24.3MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.7.0%2Bcu101/torch_cluster-1.5.8-cp36-cp36m-linux_x86_64.whl (21.5MB)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.7.0%2Bcu101/torch_spline_conv-1.2.0-cp36-cp36m-linux_x86_64.whl (6.4MB)\n",
            "Installing collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "  Downloading https://files.pythonhosted.org/packages/59/5c/3e95b76321fb14f24cc2ace392075717f645c4632e796ee0db1bc7d17231/torch_geometric-1.6.3.tar.gz (186kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.51.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/36/de17e79f29e06d9a92746d0dd9ec4636487ab03f6af10e78586aae533f7a/ase-3.21.1-py3-none-any.whl (2.2MB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (3.7.4.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (1.0.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (53.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py): started\n",
            "  Building wheel for torch-geometric (setup.py): finished with status 'done'\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.3-cp36-none-any.whl size=322720 sha256=0d0a5edc7d07109c514fddeefd0395d534a369aa2804496752e239b81fe493e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/47/1e/0af8ce3e21783c3e584c22502011a3367c091694eebc50a971\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.21.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5loTmKsiLiR"
      },
      "source": [
        "# **PYTORCH GEOMETRIC intro example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNf8JpDn3hCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c6473f-07a7-4a75-ad78-ce4eca8cd1a0"
      },
      "source": [
        "import torch\r\n",
        "from torch_geometric.data import Data\r\n",
        "\r\n",
        "# all edges in the graph with shape [ 2 , nb_edges ]\r\n",
        "# use edge_index.t().contiguous() for shape [ nb_edges , 2 ]\r\n",
        "edge_index = torch.tensor([[0, 1, 1, 2],\r\n",
        "                           [1, 0, 2, 1]], dtype=torch.long)\r\n",
        "\r\n",
        "\r\n",
        "# feature vector of each node with shape [ nb_nodes , nb_features ] ( here 1 dimension )\r\n",
        "x = torch.tensor([[-1,2], [0, 2], [1, 2]], dtype=torch.float)\r\n",
        "\r\n",
        "data = Data(x=x, edge_index=edge_index)\r\n",
        "\r\n",
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 4], x=[3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bddIh1-w3xs5",
        "outputId": "89c76d74-3951-4a58-cc36-dfbcf3697cbe"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset\r\n",
        "from torch_geometric.data import DataLoader\r\n",
        "\r\n",
        "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES' , use_node_attr=True)\r\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\r\n",
        "\r\n",
        "for batch in loader:\r\n",
        "    print(batch)\r\n",
        "    print(batch.batch) # to which graph the node index is in the batch\r\n",
        "    print(batch.num_graphs)\r\n",
        "    break\r\n",
        "\r\n",
        "# u can use : import torch_geometric.transforms as T, to modify the dataset for example :  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Extracting /tmp/ENZYMES/ENZYMES/ENZYMES.zip\n",
            "Processing...\n",
            "Done!\n",
            "Batch(batch=[1028], edge_index=[2, 4014], x=[1028, 21], y=[32])\n",
            "tensor([ 0,  0,  0,  ..., 31, 31, 31])\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRNZkw5v5czC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09d9b7e-51db-4c2a-d8d6-02f8a14145a0"
      },
      "source": [
        "from torch_geometric.datasets import Planetoid\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch_geometric.nn import GCNConv\r\n",
        "\r\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\r\n",
        "\r\n",
        "\r\n",
        "class Net(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\r\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\r\n",
        "\r\n",
        "    def forward(self, data):\r\n",
        "        x, edge_index = data.x, data.edge_index\r\n",
        "\r\n",
        "        x = self.conv1(x, edge_index)\r\n",
        "        x = F.relu(x)\r\n",
        "        x = F.dropout(x, training=self.training)\r\n",
        "        x = self.conv2(x, edge_index)\r\n",
        "\r\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0cVeJXDB8JN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c2cfd8-e18c-401e-cc45-ecc6e5220c65"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = Net().to(device)\r\n",
        "data = dataset[0].to(device)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\r\n",
        "\r\n",
        "model.train()\r\n",
        "for epoch in range(10):\r\n",
        "    optimizer.zero_grad()\r\n",
        "    out = model(data)\r\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    print(loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.9523, grad_fn=<NllLossBackward>)\n",
            "tensor(1.8488, grad_fn=<NllLossBackward>)\n",
            "tensor(1.7244, grad_fn=<NllLossBackward>)\n",
            "tensor(1.5880, grad_fn=<NllLossBackward>)\n",
            "tensor(1.4203, grad_fn=<NllLossBackward>)\n",
            "tensor(1.3007, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1425, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9185, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8286, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lz_aoN7B9w6",
        "outputId": "1979009f-a5d1-4d2f-c638-3e0baad2e0b4"
      },
      "source": [
        "model.eval()\r\n",
        "_, pred = model(data).max(dim=1)\r\n",
        "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\r\n",
        "acc = correct / int(data.test_mask.sum())\r\n",
        "print('Accuracy: {:.4f}'.format(acc))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo-H5fRPh3mz"
      },
      "source": [
        "# **MOLECULE Graph Neural Network !!!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQFgOoWbKDNw",
        "outputId": "fcd56f81-844e-4da7-fbc4-46a0be715265"
      },
      "source": [
        "!pip install pysmiles"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysmiles\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/c0/f80accbef999deefe9e3f92213b4791347356c6a69e15efdcee0f2ac22ae/pysmiles-1.0.1.tar.gz\n",
            "Collecting pbr\n",
            "  Using cached https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: networkx~=2.0 in /usr/local/lib/python3.6/dist-packages (from pysmiles) (2.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx~=2.0->pysmiles) (4.4.2)\n",
            "Building wheels for collected packages: pysmiles\n",
            "  Building wheel for pysmiles (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysmiles: filename=pysmiles-1.0.1-py2.py3-none-any.whl size=22017 sha256=1821e85274c0a22dcc131d2b1568edeef4cbe58c260b507ca520b44feb4f8606\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/ba/9b/b96056ad3f356628ebc98a6da947d9a038f4b550c84316a2e2\n",
            "Successfully built pysmiles\n",
            "Installing collected packages: pbr, pysmiles\n",
            "Successfully installed pbr-5.5.1 pysmiles-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "aeIRI3KeCNpq",
        "outputId": "2189a0a3-6794-4602-be9c-4b787e1f6d3b"
      },
      "source": [
        "from pysmiles import read_smiles\r\n",
        "import networkx as nx\r\n",
        "\r\n",
        "smiles = 'C1CC[13CH2]CC1C1CCCCC1'\r\n",
        "mol = read_smiles(smiles)\r\n",
        "\r\n",
        "# atom vector (C only)\r\n",
        "print(mol.nodes(data='element'))\r\n",
        "# adjacency matrix\r\n",
        "print(nx.to_numpy_matrix(mol))\r\n",
        "\r\n",
        "nx.draw(mol)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 'C'), (1, 'C'), (2, 'C'), (3, 'C'), (4, 'C'), (5, 'C'), (6, 'C'), (7, 'C'), (8, 'C'), (9, 'C'), (10, 'C'), (11, 'C')]\n",
            "[[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF027fEZJUDP"
      },
      "source": [
        "X_smiles = [ 'CCCCCCC'  , 'C1CC[13CH2]CC1C1CCCCC1' , 'CC' , 'C1CC[13CH2]CC1C1CCCCC1' , 'CCCCCC',\r\n",
        "           'C1CC[13CH2]CC1C1CCCCC1' , 'CCCCCCC' , 'C1CC[13CH2]CC1C1CCCCC1' , 'CCCC' , 'C1CC[13CH2]CC1C1CCCCC1',\r\n",
        "           'CCCCCCC']\r\n",
        "Y_train = [1,0,1,0,1,0,1,0,1,0,1]\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kQDn4bktoDX"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "X_train = []\r\n",
        "\r\n",
        "for j in range(len(X_smiles)):\r\n",
        "  x = X_smiles[j]\r\n",
        "  mol = read_smiles(x)\r\n",
        "\r\n",
        "  # edeges of the graph\r\n",
        "  edg_list =  list(nx.to_edgelist(mol))\r\n",
        "\r\n",
        "  final_edg = np.zeros(( 2, len(edg_list) ) )\r\n",
        "  for i in range( len(edg_list) ):\r\n",
        "    final_edg[0][i] = edg_list[i][0]\r\n",
        "    final_edg[1][i] = edg_list[i][1]\r\n",
        "\r\n",
        "  edge_index = torch.tensor(final_edg , dtype=torch.long)\r\n",
        "  edge_index\r\n",
        "\r\n",
        "  # feature of nodes\r\n",
        "  ft_nodes = list(mol.nodes(data='element'))\r\n",
        "\r\n",
        "  final_nodes = np.zeros( ( len(ft_nodes) , 3) )\r\n",
        "  for i in range( len(ft_nodes) ):  \r\n",
        "    if(ft_nodes[i][1] == 'C'):\r\n",
        "      final_nodes[i][0] = 1\r\n",
        "      final_nodes[i][1] = 2\r\n",
        "      final_nodes[i][2] = 3\r\n",
        "\r\n",
        "  x = torch.tensor(final_nodes, dtype=torch.float)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  # final \r\n",
        "  data = Data(x=x, edge_index=edge_index , y = torch.tensor(Y_train[j], dtype=torch.float) ) \r\n",
        "  X_train.append(data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DcKai43uQSe",
        "outputId": "cc3577f8-1445-4303-9e4d-79338c1c24b1"
      },
      "source": [
        "print(len(X_train) , len(Y_train) , X_train[0]  , Y_train[0] )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11 11 Data(edge_index=[2, 6], x=[7, 3], y=1.0) 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ3jhcwBZWYl"
      },
      "source": [
        "loader = DataLoader(X_train, batch_size=4)\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uofnGWEZbnf",
        "outputId": "2fe48b05-6806-4793-aee3-28b3fd10586c"
      },
      "source": [
        "for b in loader:\r\n",
        "  print(b)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch(batch=[33], edge_index=[2, 33], x=[33, 3], y=[4])\n",
            "Batch(batch=[37], edge_index=[2, 37], x=[37, 3], y=[4])\n",
            "Batch(batch=[23], edge_index=[2, 22], x=[23, 3], y=[3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m3VTrlDuSQ0"
      },
      "source": [
        "from torch_scatter import scatter_add\r\n",
        "\r\n",
        "FEATURE_NODE_NB = 3\r\n",
        "\r\n",
        "FEATURE_GNN_NB = 10\r\n",
        "\r\n",
        "\r\n",
        "class Net(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.conv1 = GCNConv( FEATURE_NODE_NB , 16)\r\n",
        "        self.conv2 = GCNConv(16, FEATURE_GNN_NB)\r\n",
        "        \r\n",
        "        self.fc1 = torch.nn.Linear( FEATURE_GNN_NB , 8)  \r\n",
        "        self.fc2 = torch.nn.Linear(8, 5)\r\n",
        "        self.fc3 = torch.nn.Linear(5, 1)\r\n",
        "\r\n",
        "    def forward(self, data):\r\n",
        "        x, edge_index = data.x, data.edge_index\r\n",
        "\r\n",
        "        x = self.conv1(x, edge_index)\r\n",
        "        x = F.relu(x)\r\n",
        "        x = F.dropout(x, training=self.training)\r\n",
        "        x = self.conv2(x, edge_index)\r\n",
        "\r\n",
        "        # care !!! sum only within same mol\r\n",
        "        if self.training:\r\n",
        "          x = scatter_add(x, data.batch, dim=0)\r\n",
        "        else: \r\n",
        "          x = torch.sum(x, dim=0)\r\n",
        "\r\n",
        "        x = F.relu( self.fc1(x) )\r\n",
        "        x = F.relu( self.fc2(x) )\r\n",
        "        x = self.fc3(x)\r\n",
        "\r\n",
        "        return x#F.log_softmax(x, dim=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1_ggBgG2tid",
        "outputId": "4aed8986-11c4-4fd2-b1df-776147359982"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = Net().to(device)\r\n",
        "print(model)\r\n",
        "#loader = DataLoader(X_train, batch_size=3)\r\n",
        "#data = loader[0].to(device)\r\n",
        "data = X_train[1]\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\r\n",
        "lossF = torch.nn.L1Loss()\r\n",
        "\r\n",
        "model.train()\r\n",
        "for epoch in range(1000):\r\n",
        "\r\n",
        "    for b in loader:\r\n",
        "      optimizer.zero_grad()\r\n",
        "      data = b\r\n",
        "      out = model(data)\r\n",
        "\r\n",
        "      # compute loss and backprop\r\n",
        "      loss = lossF( torch.squeeze(out) , data.y )\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "    \r\n",
        "    print(loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): GCNConv(3, 16)\n",
            "  (conv2): GCNConv(16, 10)\n",
            "  (fc1): Linear(in_features=10, out_features=8, bias=True)\n",
            "  (fc2): Linear(in_features=8, out_features=5, bias=True)\n",
            "  (fc3): Linear(in_features=5, out_features=1, bias=True)\n",
            ")\n",
            "tensor(0.8960, grad_fn=<L1LossBackward>)\n",
            "tensor(0.6426, grad_fn=<L1LossBackward>)\n",
            "tensor(0.6239, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5919, grad_fn=<L1LossBackward>)\n",
            "tensor(0.6277, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5926, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5745, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5538, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5537, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5676, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5028, grad_fn=<L1LossBackward>)\n",
            "tensor(0.6219, grad_fn=<L1LossBackward>)\n",
            "tensor(0.6238, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5309, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4663, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5136, grad_fn=<L1LossBackward>)\n",
            "tensor(0.5191, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4732, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4753, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4952, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4367, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4609, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4389, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4448, grad_fn=<L1LossBackward>)\n",
            "tensor(0.3791, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4301, grad_fn=<L1LossBackward>)\n",
            "tensor(0.3182, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4120, grad_fn=<L1LossBackward>)\n",
            "tensor(0.3909, grad_fn=<L1LossBackward>)\n",
            "tensor(0.3162, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2921, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4275, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2889, grad_fn=<L1LossBackward>)\n",
            "tensor(0.3564, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2501, grad_fn=<L1LossBackward>)\n",
            "tensor(0.3088, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2262, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2310, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1420, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2639, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2425, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1586, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2715, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2219, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2405, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1417, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2534, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1761, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2408, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1614, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1047, grad_fn=<L1LossBackward>)\n",
            "tensor(0.4071, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1969, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2274, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1564, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1357, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0466, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1533, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1752, grad_fn=<L1LossBackward>)\n",
            "tensor(0.3306, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2666, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1487, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1543, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1972, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2406, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2369, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1084, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1385, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2382, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1982, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2181, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1506, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1311, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1926, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0635, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1619, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0984, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0851, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1532, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1870, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0780, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2024, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1522, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1276, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0275, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1012, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0730, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0881, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1729, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1749, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1526, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1752, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0754, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1207, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1296, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1394, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1384, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0310, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1056, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1850, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2010, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0473, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0558, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1910, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0510, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1205, grad_fn=<L1LossBackward>)\n",
            "tensor(0.2115, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0554, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1815, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0658, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0539, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0627, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0343, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1617, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0778, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0902, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1406, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0324, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0918, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0540, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1194, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0083, grad_fn=<L1LossBackward>)\n",
            "tensor(0.1116, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0450, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0192, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0893, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0653, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0792, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0209, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0539, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0814, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0197, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0370, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0316, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0217, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0133, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0698, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0489, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0388, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0165, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0752, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0563, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0404, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0251, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0900, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0928, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0441, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0650, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0113, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0323, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0418, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0259, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0263, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0083, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0134, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0215, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0202, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0285, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0143, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0079, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0103, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0092, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0254, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0493, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0140, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0096, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0137, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0231, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0226, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0258, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0039, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0100, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0127, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0151, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0228, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0190, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0121, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0214, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0125, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0263, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0489, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0129, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0381, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0503, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0069, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0048, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0242, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0130, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0094, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0118, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0203, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0067, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0040, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0026, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0045, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0066, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0079, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0241, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0255, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0086, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0056, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0102, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0028, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0135, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0214, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0098, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0013, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0066, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0157, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0224, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0120, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0187, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0133, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0117, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0035, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0076, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0207, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0092, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0031, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0204, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0061, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0134, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0109, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0023, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0052, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0195, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0241, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0159, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0213, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0197, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0174, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0180, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0134, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0062, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0055, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0167, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0123, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0208, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0087, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0179, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0178, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0048, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0265, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0038, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0132, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0077, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0228, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0194, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0146, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0077, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0067, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0059, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0178, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0130, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0221, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0197, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0245, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0317, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0041, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0412, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0360, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0302, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0220, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0426, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0188, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0326, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0305, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0290, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0258, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0076, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0054, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0105, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0084, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0189, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0044, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0045, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0123, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0219, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0026, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0269, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0455, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0110, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0346, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0306, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0175, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0124, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0257, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0268, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0290, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0311, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0500, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0200, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0419, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0241, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0212, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0152, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0027, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0132, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0069, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0125, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0015, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0050, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0151, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0237, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0243, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0039, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0069, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0194, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0041, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0061, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0095, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0143, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0085, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0154, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0114, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0127, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0057, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0055, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0309, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0236, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0289, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0310, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0467, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0215, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0348, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0354, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0151, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0211, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0044, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0192, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0163, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0102, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0134, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0049, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0058, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0123, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0103, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0168, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0117, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0059, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0129, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0262, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0171, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0146, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0010, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0036, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0230, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0328, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0097, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0032, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0031, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0039, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0023, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0049, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0012, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0078, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0197, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0188, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0141, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0217, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0171, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0205, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0095, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0228, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0077, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0145, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0087, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0020, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0134, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0025, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0232, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0172, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0085, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0077, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0085, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0038, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0143, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0020, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0030, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0223, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0132, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0094, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0056, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0062, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0184, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0381, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0366, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0513, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0480, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0519, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0554, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0495, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0276, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0133, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0116, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0084, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0033, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0116, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0166, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0042, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0179, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0050, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0101, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0115, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0066, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0208, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0061, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0192, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0444, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0049, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0422, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0306, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0100, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0217, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0332, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0176, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0148, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0216, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0319, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0147, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0145, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0030, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0101, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0042, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0078, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0171, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0073, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0026, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0042, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0230, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0114, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0017, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0086, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0102, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0067, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0123, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0145, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0028, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0201, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0119, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0008, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0108, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0238, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0221, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0045, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0151, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0076, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0122, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0152, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0066, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0151, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0075, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0167, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0082, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0036, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0169, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0079, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0119, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0021, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0062, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0113, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0248, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0085, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0021, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0110, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0169, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0080, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0172, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0132, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0069, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0085, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0120, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0240, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0274, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0106, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0331, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0183, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0067, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0222, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0089, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0080, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0164, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0104, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0165, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0188, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0162, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0143, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0170, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0125, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0015, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0040, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0131, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0089, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0086, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0117, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0067, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0145, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0067, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0135, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0028, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0098, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0106, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0065, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0044, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0103, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0125, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0040, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0055, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0035, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0076, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0041, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0056, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0153, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0066, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0203, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0231, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0314, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0052, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0382, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0250, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0178, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0159, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0026, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0052, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0183, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0208, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0119, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0217, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0089, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0210, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0193, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0125, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0146, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0203, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0262, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0188, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0267, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0293, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0188, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0272, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0379, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0155, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0173, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0076, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0022, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0086, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0141, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0069, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0172, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0138, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0242, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0158, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0133, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0093, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0096, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0183, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0234, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0298, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0348, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0306, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0166, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0215, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0044, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0084, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0141, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0022, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0050, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0087, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0170, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0093, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0038, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0058, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0139, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0055, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0129, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0130, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0054, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0147, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0027, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0008, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0084, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0207, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0096, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0183, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0063, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0031, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0100, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0094, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0119, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0144, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0102, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0037, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0076, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0189, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0181, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0013, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0063, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0087, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0057, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0157, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0088, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0015, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0037, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0160, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0183, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0144, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0156, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0199, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0083, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0083, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0037, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0119, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0315, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0179, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0043, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0362, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0353, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0236, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0185, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0224, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0271, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0264, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0365, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0173, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0100, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0103, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0162, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0039, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0161, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0155, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0088, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0206, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0179, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0063, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0059, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0059, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0094, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0098, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0021, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0077, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0141, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0108, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0058, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0185, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0052, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0160, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0025, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0173, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0049, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0186, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0057, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0155, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0115, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0041, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0018, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0073, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0096, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0128, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0085, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0132, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0099, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0049, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0064, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0080, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0149, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0078, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0109, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0027, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0078, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0113, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0196, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0184, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0186, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0147, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0103, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0019, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0058, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0113, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0139, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0114, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0033, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0094, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0100, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0036, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0024, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0151, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0082, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0105, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0031, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0064, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0074, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0041, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0082, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0047, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0074, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0113, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0075, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0043, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0127, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0279, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0163, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0029, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0088, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0077, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0138, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0239, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0181, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0077, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0296, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0237, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0235, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0313, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0318, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0221, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0290, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0280, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0138, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0112, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0089, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0104, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0105, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0215, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0121, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0179, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0081, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0033, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0092, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0099, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0259, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0154, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0133, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0045, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0117, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0088, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0060, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0037, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0193, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0405, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0299, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0480, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0203, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0212, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0058, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0039, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0152, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0347, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0285, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0360, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0358, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0120, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0047, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0113, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0255, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0193, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0122, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0169, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0045, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0059, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0035, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0037, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0055, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0044, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0015, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0040, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0067, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0200, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0043, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0304, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0237, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0245, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0322, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0327, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0158, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0230, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0372, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0186, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0257, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0394, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0078, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0116, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0029, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0072, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0051, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0093, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0017, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0119, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0073, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0011, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0030, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0079, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0160, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0084, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0124, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0083, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0016, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0056, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0116, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0244, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0316, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0131, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0295, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0347, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0218, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0182, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0294, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0150, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0164, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0066, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0056, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0137, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0062, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0183, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0165, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0394, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0041, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0381, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0291, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0017, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0097, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0090, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0116, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0103, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0120, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0024, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0222, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0296, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0113, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0012, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0208, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0054, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0252, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0200, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0095, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0224, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0261, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0326, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0294, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0139, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0143, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0063, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0044, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0083, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0069, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0018, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0033, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0144, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0083, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0155, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0100, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0186, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0158, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0028, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0081, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0121, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0068, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0015, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0032, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0039, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0059, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0159, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0086, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0013, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0124, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0145, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0291, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0287, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0406, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0359, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0278, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0192, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0279, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0220, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0330, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0055, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0053, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0188, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0143, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0012, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0048, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0142, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0106, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0020, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0156, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0070, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0112, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0091, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0056, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0153, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0047, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0198, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0355, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0046, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0354, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0337, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0058, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0348, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0159, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0174, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0131, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0080, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0081, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0161, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0089, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0147, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0333, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0191, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0264, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0437, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0121, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0222, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0277, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0151, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0114, grad_fn=<L1LossBackward>)\n",
            "tensor(0.0120, grad_fn=<L1LossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veEydzYk4w9d"
      },
      "source": [
        "SAVE_PATH = 'test_model.pth'\r\n",
        "\r\n",
        "torch.save(model.state_dict()  , SAVE_PATH)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l_hyBoEt8bx",
        "outputId": "a3bc1d07-be5e-45b4-ed59-cc980691a9d3"
      },
      "source": [
        "model_l = Net()\r\n",
        "model_l.load_state_dict(torch.load(SAVE_PATH))\r\n",
        "model_l.eval()\r\n",
        "\r\n",
        "model_l(X_train[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0166], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYc1GqWzuljc"
      },
      "source": [
        "#"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W8eOoHiySjI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iAgEeET1GgP"
      },
      "source": [
        ""
      ]
    }
  ]
}